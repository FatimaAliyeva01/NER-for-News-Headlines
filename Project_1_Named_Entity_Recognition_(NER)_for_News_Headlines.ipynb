{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Named Entity Recognition (NER) for News Headlines**\n",
        "\n",
        "## **1. Introduction**\n",
        "   - **Project Overview:** This project focuses on developing a Named Entity Recognition (NER) system to identify and classify entities such as persons, organizations, and locations within news headlines. NER is a key task in Natural Language Processing (NLP), particularly useful in extracting meaningful information from unstructured text data.\n",
        "   ---\n",
        "   - **Objectives:**\n",
        "     - Implement an NER model using the CoNLL-2003 dataset.\n",
        "     - Preprocess the data to prepare it for model training.\n",
        "     - Train and fine-tune a model using spaCy’s small English model.\n",
        "     - Evaluate the model using standard NLP metrics.\n",
        "     - Apply the model to new headlines for practical insights."
      ],
      "metadata": {
        "id": "1HdNOOYiH9F7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Dataset Description**\n",
        "   - **Dataset:** The CoNLL-2003 dataset is a well-known benchmark dataset in NLP, specifically designed for NER tasks. It includes annotated text data with entities categorized as:\n",
        "     - `PER` (Person)\n",
        "     - `ORG` (Organization)\n",
        "     - `LOC` (Location)\n",
        "     - `MISC` (Miscellaneous)\n",
        "---\n",
        "   - **Data Structure:**\n",
        "     - The dataset is organized into sentences, where each word is tagged with its corresponding entity type or labeled as `O` if it does not belong to any named entity.\n",
        "     - The dataset is split into training, validation, and test sets to facilitate model development and evaluation."
      ],
      "metadata": {
        "id": "Bzxbu4p6IIsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **3. Data Preprocessing and Exploration**\n",
        "---\n",
        "   - **Data Loading:** Load the CoNLL-2003 dataset using appropriate libraries, ensuring that it is ready for processing.\n",
        "   - **Data Cleaning:**\n",
        "     - Convert the raw data into a structured format suitable for model training.\n",
        "     - Tokenize sentences and align tokens with their respective entity tags.\n",
        "     - Handle missing data, punctuation, and any inconsistencies.\n",
        "   - **Exploratory Data Analysis (EDA):**\n",
        "     - Analyze the distribution of entity types in the dataset.\n",
        "     - Visualize the frequency of entities to understand the dataset's characteristics and potential challenges.\n",
        "\n"
      ],
      "metadata": {
        "id": "W80xpfEDIUuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3.1 Library Installation and Importing**\n",
        "\n",
        "- This section installs and imports the required libraries. Datasets is used to load the CoNLL-2003 dataset, which contains annotated data for Named Entity Recognition (NER). We also import spacy for NLP tasks, including model training and evaluation."
      ],
      "metadata": {
        "id": "bsqJ9jz_6gOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install datasets\n",
        "\n",
        "# Import necessary libraries for the project\n",
        "from datasets import load_dataset  # For loading the CoNLL-2003 dataset\n",
        "from collections import Counter  # For counting elements\n",
        "from spacy.training import Example  # For creating training examples in spaCy\n",
        "from sklearn.metrics import classification_report  # For evaluating the model's performance\n",
        "import spacy  # For NLP and NER tasks\n",
        "from spacy.training import Example  # For creating training examples in spaCy\n",
        "import random  # For shuffling data during training\n",
        "import pandas as pd  # For data manipulation and analysis\n",
        "import matplotlib.pyplot as plt  # For creating visualizations\n",
        "import seaborn as sns  # For enhancing visualizations\n",
        "import numpy as np  # For numerical operations\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1vuGLI5M6uxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.2 Loading the CoNLL-2003 Dataset**\n",
        "\n",
        "- Here, the CoNLL-2003 dataset is loaded and split into training and testing sets. This dataset contains labeled data used for training and evaluating the NER model."
      ],
      "metadata": {
        "id": "UdHenJeG7T-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CoNLL-2003 dataset, focusing on English\n",
        "dataset = load_dataset(\"conll2003\")\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_data = dataset[\"train\"]\n",
        "test_data = dataset[\"test\"]\n",
        "\n",
        "# Display the overall structure of the dataset\n",
        "print(dataset)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EHOOcpK37gSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.3 Dataset Exploration**\n",
        "- This section allows you to explore the dataset by checking the features and structure. It helps in understanding the format and the available labels (NER tags)."
      ],
      "metadata": {
        "id": "E7R083TO79N3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the structure of the dataset and inspect features\n",
        "print(dataset[\"train\"].features[\"ner_tags\"])\n",
        "print(dataset['train'].description)\n",
        "print(dataset.keys())\n",
        "print(dataset.shape)"
      ],
      "metadata": {
        "id": "sdgqsok4_ls-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the distribution of NER labels\n",
        "entity_types = [entity for sublist in train_data['ner_tags'] for entity in sublist]\n",
        "entity_counts = Counter(entity_types)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(entity_counts.keys(), entity_counts.values())\n",
        "plt.title('Distribution of Named Entity Types in the Training Set')\n",
        "plt.xlabel('Entity Type')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MLfQL-xHrgAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Model Development**\n",
        "---\n",
        "   - **Model Selection:**\n",
        "     - SpaCy's small English model is chosen for its balance between speed and accuracy.\n",
        "     - Discuss why this model is suitable for NER tasks, especially in the context of news headlines.\n",
        "   - **Training Process:**\n",
        "     - Fine-tune the pre-trained spaCy model using the CoNLL-2003 dataset.\n",
        "     - Optimize hyperparameters such as learning rate, batch size, and the number of epochs to improve performance.\n",
        "     - Monitor training metrics to ensure the model is learning effectively."
      ],
      "metadata": {
        "id": "yPMuXWdrI7v8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4.1 NER Tag Conversion**\n",
        "\n",
        "- The numerical NER tags in the dataset are converted to their corresponding text labels for better readability and understanding"
      ],
      "metadata": {
        "id": "taKC1ADO56F4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert NER label numeric values to text labels\n",
        "ner_feature = dataset[\"train\"].features[\"ner_tags\"].feature\n",
        "label_converter = ner_feature.int2str\n"
      ],
      "metadata": {
        "id": "GgvI37zoBTGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4.2 Data Transformation for Model Training**\n",
        "\n",
        "- This function converts the dataset into a format that is compatible with the spaCy training process. It prepares the data by converting each sentence into a list of tuples, where each tuple contains the token, its POS tag, chunk tag, and the corresponding NER label."
      ],
      "metadata": {
        "id": "tRVRudUz9cMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to transform the data into a format suitable for training the model\n",
        "def transform_data_for_training(data):\n",
        "    transformed_data = []\n",
        "    for sentence in data:\n",
        "        sentence_formatted = []\n",
        "        for token, pos_tag, chunk_tag, ner_tag in zip(sentence[\"tokens\"], sentence[\"pos_tags\"], sentence[\"chunk_tags\"], sentence[\"ner_tags\"]):\n",
        "            sentence_formatted.append((token, pos_tag, chunk_tag, label_converter(ner_tag)))\n",
        "        transformed_data.append(sentence_formatted)\n",
        "    return transformed_data\n",
        "\n",
        "# Transform the training and test datasets\n",
        "formatted_train_data = transform_data_for_training(train_data)\n",
        "formatted_test_data = transform_data_for_training(test_data)\n",
        "\n",
        "# Print a sample of the formatted data\n",
        "print(formatted_train_data[0])\n",
        "print(formatted_test_data)\n"
      ],
      "metadata": {
        "id": "6q0vu7yd8_zQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing sentence lengths after transformation\n",
        "def visualize_sentence_lengths(formatted_data, data_type=\"Train\"):\n",
        "    sentence_lengths = [len(sentence) for sentence in formatted_data]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(sentence_lengths, bins=30, color='lightcoral', edgecolor='black')\n",
        "    plt.title(f'Distribution of Sentence Lengths in {data_type} Data After Transformation')\n",
        "    plt.xlabel('Number of Tokens')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n",
        "# Visualize sentence lengths for training data\n",
        "visualize_sentence_lengths(formatted_train_data, data_type=\"Train\")\n",
        "\n",
        "# Visualize sentence lengths for test data\n",
        "visualize_sentence_lengths(formatted_test_data, data_type=\"Test\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wzx3-u0qtV0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**4.3 Converting Data to spaCy Format**\n",
        "\n",
        "- This function further processes the data into a format specifically required by spaCy for training the NER model. Each sentence is converted into a format where entities are marked with their respective labels and positions."
      ],
      "metadata": {
        "id": "bUKZ1sgX9mpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the formatted data to a format compatible with spaCy for NER training\n",
        "def convert_to_spacy_format(data):\n",
        "    spacy_data = []\n",
        "    for sentence in data:\n",
        "        words = [token for token, pos_tag, chunk_tag, label in sentence]\n",
        "        entities = []\n",
        "        start = 0\n",
        "        for token, pos_tag, chunk_tag, label in sentence:\n",
        "            end = start + len(token)\n",
        "            if label != 'O':  # 'O' indicates non-entity tokens\n",
        "                entities.append((start, end, label))\n",
        "            start = end + 1  # Move to the next token\n",
        "        spacy_data.append((' '.join(words), {\"entities\": entities}))\n",
        "    return spacy_data\n",
        "\n",
        "# Convert the training and test data\n",
        "train_data_spacy = convert_to_spacy_format(formatted_train_data)\n",
        "test_data_spacy = convert_to_spacy_format(formatted_test_data)\n",
        "\n",
        "# Print a sample of the data in spaCy format\n",
        "print(train_data_spacy[0])"
      ],
      "metadata": {
        "id": "W9j3geiA922_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.4 Model Training**\n",
        "\n",
        "- This section loads spaCy's small English model and trains it on the processed dataset. The model is trained over multiple epochs, with the NER pipeline enabled and other pipelines disabled. The losses are printed at the end of each epoch to monitor the training process"
      ],
      "metadata": {
        "id": "cdp3Rlki-E5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the small English model from spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Disable all pipes except the 'ner' pipe\n",
        "pipes_to_disable = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "nlp.disable_pipes(*pipes_to_disable)\n",
        "\n",
        "# Set up the optimizer for training\n",
        "optimizer = nlp.create_optimizer()\n",
        "\n",
        "# Training process\n",
        "for epoch in range(3):  # Train for a specific number of epochs (iterations)\n",
        "    random.shuffle(train_data_spacy)  # Shuffle training data at the beginning of each epoch\n",
        "    epoch_losses = {}  # Dictionary to keep track of losses for the current epoch\n",
        "\n",
        "    # Update the model for each training example\n",
        "    for sentence, labels in train_data_spacy:\n",
        "        doc = nlp.make_doc(sentence)  # Create a Doc object from the sentence\n",
        "        example = Example.from_dict(doc, labels)  # Convert the sentence and labels to an Example\n",
        "        nlp.update([example], drop=0.5, sgd=optimizer, losses=epoch_losses)  # Update the model with the Example\n",
        "\n",
        "    # Print losses at the end of each epoch\n",
        "    print(f\"Epoch {epoch + 1} - Losses: {epoch_losses}\")"
      ],
      "metadata": {
        "id": "871PgyxP-pEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Model Evaluation**\n",
        "---\n",
        "   - **Evaluation Metrics:**\n",
        "     - **Precision:** The proportion of correctly identified entities out of all entities identified by the model.\n",
        "     - **Recall:** The proportion of correctly identified entities out of all actual entities in the dataset.\n",
        "     - **F1-Score:** The harmonic mean of precision and recall, providing a balanced evaluation metric.\n",
        "   - **Confusion Matrix:**\n",
        "     - A confusion matrix is used to visualize the model’s performance across different entity types.\n",
        "     - Highlight any misclassifications and discuss potential reasons.\n",
        "\n"
      ],
      "metadata": {
        "id": "9Cj8giHzJUTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.1 Model Evaluation**\n",
        "\n",
        "- This function evaluates the trained NER model by comparing the predicted labels against the true labels in the test dataset. The evaluation metrics (precision, recall, and F1-score) are printed for each entity class, providing insight into the model's performance\n"
      ],
      "metadata": {
        "id": "0mUQs3F8-y4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ner_model_evaluation(nlp_model, dataset):\n",
        "    real_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    for sentence in dataset:\n",
        "        # Split the data and extract real labels\n",
        "        tokens = [word for word, pos_tag, chunk_tag, ner_label in sentence]\n",
        "        real_labels.extend([ner_label for word, pos_tag, chunk_tag, ner_label in sentence])\n",
        "\n",
        "        # Process the text with the model\n",
        "        processed_doc = nlp_model(' '.join(tokens))\n",
        "\n",
        "        # Collect predicted labels\n",
        "        for token in processed_doc:\n",
        "            if token.ent_iob_ == 'O':\n",
        "                predicted_labels.append('O')\n",
        "            else:\n",
        "                predicted_labels.append(token.ent_type_)\n",
        "\n",
        "        # Balance length differences\n",
        "        if len(predicted_labels) < len(real_labels):\n",
        "            predicted_labels.extend(['O'] * (len(real_labels) - len(predicted_labels)))\n",
        "        elif len(predicted_labels) > len(real_labels):\n",
        "            predicted_labels = predicted_labels[:len(real_labels)]\n",
        "\n",
        "    # Evaluate performance with metrics and print\n",
        "    report = classification_report(real_labels, predicted_labels)\n",
        "    print(report)\n",
        "\n",
        "# Evaluate using test data\n",
        "ner_model_evaluation(nlp, formatted_test_data)\n"
      ],
      "metadata": {
        "id": "yE8JV4lzcLqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.2 Final Model Evaluation**\n",
        "\n",
        "- This function provides a detailed evaluation of the NER model, including precision, recall, and F1-scores for each entity type. It also calculates macro and weighted averages, giving a comprehensive view of the model's performance across all classes\n"
      ],
      "metadata": {
        "id": "UzxmbSRS_Yfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics(report):\n",
        "    metrics = ['precision', 'recall', 'f1-score']\n",
        "    labels = [label for label in report.keys() if label not in ['accuracy', 'macro avg', 'weighted avg']] # Exclude the 'accuracy', 'macro avg' and 'weighted avg' keys\n",
        "\n",
        "    for metric in metrics:\n",
        "        values = [report[label][metric] if label in report else 0 for label in labels]\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        sns.barplot(x=labels, y=values, palette='viridis')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.title(f'{metric.capitalize()} for Each Entity Type')\n",
        "        plt.xlabel('Entity Type')\n",
        "        plt.ylabel(metric.capitalize())\n",
        "        plt.show()\n",
        "\n",
        "def detailed_ner_model_evaluation(nlp_model, dataset):\n",
        "    real_labels = []\n",
        "    predicted_labels = []\n",
        "    for sentence in dataset:\n",
        "        tokens = [word for word, pos_tag, chunk_tag, ner_label in sentence]\n",
        "        real_labels.extend([ner_label for word, pos_tag, chunk_tag, ner_label in sentence])\n",
        "        processed_doc = nlp_model(' '.join(tokens))\n",
        "        for token in processed_doc:\n",
        "            if token.ent_iob_ == 'O':\n",
        "                predicted_labels.append('O')\n",
        "            else:\n",
        "                predicted_labels.append(token.ent_type_)\n",
        "        if len(predicted_labels) < len(real_labels):\n",
        "            predicted_labels.extend(['O'] * (len(real_labels) - len(predicted_labels)))\n",
        "        elif len(predicted_labels) > len(real_labels):\n",
        "            predicted_labels = predicted_labels[:len(real_labels)]\n",
        "    report = classification_report(real_labels, predicted_labels, output_dict=True)\n",
        "    classes = ['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n",
        "    for label in classes:\n",
        "        if label in report:\n",
        "            print(f\"{label}:\\n\"\n",
        "                  f\"  Precision: {report[label]['precision']:.2f}\\n\"\n",
        "                  f\"  Recall: {report[label]['recall']:.2f}\\n\"\n",
        "                  f\"  F1-Score: {report[label]['f1-score']:.2f}\\n\"\n",
        "                  f\"  Support: {report[label]['support']}\\n\")\n",
        "    print(f\"Accuracy: {report['accuracy']:.2f}\")\n",
        "    print(f\"Macro Avg - Precision: {report['macro avg']['precision']:.2f}\")\n",
        "    print(f\"Macro Avg - Recall: {report['macro avg']['recall']:.2f}\")\n",
        "    print(f\"Macro Avg - F1-Score: {report['macro avg']['f1-score']:.2f}\")\n",
        "    print(f\"Weighted Avg - Precision: {report['weighted avg']['precision']:.2f}\")\n",
        "    print(f\"Weighted Avg - Recall: {report['weighted avg']['recall']:.2f}\")\n",
        "    print(f\"Weighted Avg - F1-Score: {report['weighted avg']['f1-score']:.2f}\")\n",
        "\n",
        "    # Call plot_metrics within detailed_ner_model_evaluation to access real_labels and predicted_labels\n",
        "    plot_metrics(report)\n",
        "\n",
        "# Assuming nlp and formatted_test_data are defined elsewhere\n",
        "# Evaluate the model with detailed metrics\n",
        "detailed_ner_model_evaluation(nlp, formatted_test_data)"
      ],
      "metadata": {
        "id": "RNUfqPmrfvJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Named Entity Recognition on New Headlines**\n",
        "---\n",
        "   - **Implementation:**\n",
        "     - Develop a function to apply the trained NER model to new, unseen headlines.\n",
        "     - Ensure the function is user-friendly and can handle various input formats.\n",
        "\n",
        "   - **Example Predictions:**\n",
        "     - Demonstrate the model's predictions on a set of new headlines.\n",
        "     - Analyze the results to showcase the model's ability to generalize to real-world data.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fpGfUU8bJxtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_ner_on_headlines(headlines):\n",
        "    \"\"\"\n",
        "    Perform Named Entity Recognition (NER) on a list of news headlines.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    headlines : list of str\n",
        "        A list of news headlines to analyze.\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    list of dict\n",
        "        A list of dictionaries where each dictionary contains:\n",
        "        - 'headline': The original headline (str).\n",
        "        - 'entities': A list of tuples with identified entities and their labels (list of tuples).\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize an empty list to store the results\n",
        "    results = []\n",
        "\n",
        "    # Process each headline\n",
        "    for headline in headlines:\n",
        "        # Use the spaCy model to process the headline\n",
        "        doc = nlp(headline)\n",
        "\n",
        "        # Extract entities and their labels from the processed document\n",
        "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "        # Create a dictionary for the headline and its identified entities\n",
        "        result = {\n",
        "            'headline': headline,\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "        # Append the result to the results list\n",
        "        results.append(result)\n",
        "\n",
        "    # Return the list of results\n",
        "    return results\n",
        "\n",
        "# Example usage:\n",
        "headlines = [\n",
        "    \"Apple is looking at buying U.K. startup for $1 billion.\",\n",
        "    \"Barack Obama will be visiting Germany next week.\",\n",
        "    \"Amazon opens a new office in San Francisco.\"\n",
        "]\n",
        "\n",
        "# Perform NER on the provided headlines\n",
        "ner_results = perform_ner_on_headlines(headlines)\n",
        "\n",
        "# Print the results\n",
        "for result in ner_results:\n",
        "    print(f\"Headline: {result['headline']}\")\n",
        "    print(\"Entities:\")\n",
        "    for entity in result['entities']:\n",
        "        print(f\"  - {entity[0]}: {entity[1]}\")\n",
        "    print()  # Add a blank line for readability\n"
      ],
      "metadata": {
        "id": "sHfPCB0bhm8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Conclusion**\n",
        "- This project has successfully demonstrated the development of a cutting-edge NER system with high accuracy and practical applicability. By fine-tuning spaCy’s pre-trained model on the CoNLL-2003 dataset, we have created a powerful tool for named entity recognition. The impressive performance and real-world application confirm the model’s effectiveness and potential for significant contributions to natural language processing. Moving forward, continued refinement and exploration of broader applications will further enhance the model’s capabilities and impact.\n"
      ],
      "metadata": {
        "id": "synKf6BQnnOb"
      }
    }
  ]
}